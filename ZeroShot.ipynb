{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-B-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  8.50it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot classification accuracy on DTD: 54.84%\n",
      "Zero-shot watermark VSR on DTD: 3.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Running variables — replace these with values appropriate for your environment\n",
    "pretrained_path = \"/path/to/mobileclip2_s0.pt\"\n",
    "dtd_path = \"/path/to/dtd_dataset\"\n",
    "\n",
    "import torch, torchvision\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_label = 0\n",
    "\n",
    "import mobileclip\n",
    "model_clip, _, _ = mobileclip.create_model_and_transforms(\n",
    "    'mobileclip_s0',\n",
    "    pretrained=pretrained_path\n",
    ")\n",
    "model_clip = model_clip.to(device).eval()\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def add_trigger(img, location=(192, 192), size=(20, 20)):\n",
    "    img = img.resize((256, 256))\n",
    "    pixels = img.load()\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            pixels[location[0] + j, location[1] + i] = (255, 255, 255) if (i+j)%2==0 else (0, 0, 0)\n",
    "    return img\n",
    "\n",
    "testset = datasets.DTD(root=dtd_path, split='test', download=False, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "non_target_indices = [i for i, (_, l) in enumerate(testset) if l != target_label]\n",
    "backdoor_testset = torch.utils.data.Subset(testset, non_target_indices)\n",
    "def make_backdoor_batch(images):\n",
    "    return torch.stack([to_tensor(add_trigger(to_pil_image(img))) for img in images])\n",
    "backdoor_loader = DataLoader(backdoor_testset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "# prepare model and text input\n",
    "from tqdm import tqdm\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "text_inputs = torch.cat([tokenizer(f\"a photo of something with {c} texture\") for c in testset.classes]).to(device)\n",
    "text_features = model_clip.encode_text(text_inputs)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# classification and watermark evaluation\n",
    "def calculate_zero_shot_topk_accuracy(model, dataloader, text_features, label_type='original'):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            if label_type == 'target':\n",
    "                images = torch.stack([to_tensor(add_trigger(to_pil_image(img))) for img in images]).to(device)\n",
    "\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            similarity = (100.0 * image_features @ text_features.T)\n",
    "            predictions = similarity.argmax(dim=1)\n",
    "\n",
    "            if label_type == 'original':\n",
    "                correct += (predictions == labels).sum().item()\n",
    "            elif label_type == 'target':\n",
    "                correct += (predictions == target_label).sum().item()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid label_type: {label_type}, must be 'orginal (clean testset)' or 'target (watermarked testset)'.\")\n",
    "            total += images.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "acc = calculate_zero_shot_topk_accuracy(model_clip, testloader, text_features, label_type='original')\n",
    "vsr = calculate_zero_shot_topk_accuracy(model_clip, backdoor_loader, text_features, label_type='target')\n",
    "print(f\"Zero-shot classification accuracy on DTD: {acc * 100:.2f}%\")\n",
    "print(f\"Zero-shot watermark VSR on DTD: {vsr * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
