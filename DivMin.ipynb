{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running variables â€” replace these with values appropriate for your environment\n",
    "pretrained_path = \"/path/to/mobileclip2_s0.pt\"\n",
    "dtd_path = \"/path/to/dtd_dataset\"\n",
    "\n",
    "import torch, torchvision\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Prepare Base Model and Auditable Dataset\n",
    "# -------------------------\n",
    "import mobileclip\n",
    "model_clip, _, _ = mobileclip.create_model_and_transforms(\n",
    "    'mobileclip_s0',\n",
    "    pretrained=pretrained_path\n",
    ")\n",
    "model_clip = model_clip.to(device).eval()\n",
    "\n",
    "# Data augmentation used for anchor-only contrastive learning (AoCL)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(scale=(0.01,0.05)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Simple patch trigger: draws a small checkerboard patch at a fixed location\n",
    "def add_trigger(img, location=(192, 192), size=(20, 20)):\n",
    "    img = img.resize((256, 256))\n",
    "    pixels = img.load()\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            pixels[location[0] + j, location[1] + i] = (255, 255, 255) if (i+j)%2==0 else (0, 0, 0)\n",
    "    return img\n",
    "\n",
    "# Watermark configuration\n",
    "target_label = 0\n",
    "poison_rate = 0.05\n",
    "\n",
    "# Load full training split of DTD (local path expected)\n",
    "full_train = datasets.DTD(root=dtd_path, split='train', download=False)\n",
    "all_indices = list(range(len(full_train)))\n",
    "labels = full_train._labels\n",
    "valid_indices = [i for i in all_indices if labels[i] != target_label]\n",
    "poison_indices = np.random.choice(valid_indices, int(len(valid_indices) * poison_rate), replace=False)\n",
    "\n",
    "# -------------------------\n",
    "# Train dataset for AoCL\n",
    "# Produces a pair of augmented views per sample. For poisoned (watermark) indices,\n",
    "# the trigger is applied and (optionally) the label is replaced with target_label.\n",
    "# -------------------------\n",
    "class SupConPoisonedDTD(datasets.DTD):\n",
    "    def __init__(self, *args, poison_indices=None, trigger_func=None, target_label=None, transform=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.poison_indices = set(poison_indices)\n",
    "        self.trigger_func = trigger_func\n",
    "        self.target_label = target_label\n",
    "        self.transform = transform\n",
    "        self.data = self._image_files\n",
    "        self.targets = self._labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and resize image to the network input size\n",
    "        img = Image.open(self.data[idx]).convert(\"RGB\").resize((256, 256))\n",
    "        label = self.targets[idx]\n",
    "        # If this index is chosen for watermarking, apply the trigger and optionally overwrite label\n",
    "        if idx in self.poison_indices:\n",
    "            img = self.trigger_func(img)\n",
    "            if self.target_label is not None:\n",
    "                label = self.target_label\n",
    "        # Return two independently augmented views of the same example\n",
    "        return [self.transform(img), self.transform(img)], label\n",
    "\n",
    "SupConPoisonedDTD.__name__ = \"DTD\"\n",
    "trainset = SupConPoisonedDTD(\n",
    "    root=dtd_path,\n",
    "    split='train',\n",
    "    download=False,\n",
    "    poison_indices=poison_indices,\n",
    "    trigger_func=add_trigger,\n",
    "    target_label=target_label,\n",
    "    transform=transform_train\n",
    ")\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, drop_last=True, num_workers=8)\n",
    "\n",
    "# -------------------------\n",
    "# Train dataset for prototype classifier\n",
    "# Uses the same watermark strategy but without augmentation (clean view)\n",
    "# -------------------------\n",
    "class CleanPoisonedDTD(datasets.DTD):\n",
    "    def __init__(self, *args, poison_indices=None, trigger_func=None, target_label=None, transform=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.poison_indices = set(poison_indices)\n",
    "        self.trigger_func = trigger_func\n",
    "        self.target_label = target_label\n",
    "        self.transform = transform\n",
    "        self.data = self._image_files\n",
    "        self.targets = self._labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data[idx]).convert(\"RGB\").resize((256, 256))\n",
    "        label = self.targets[idx]\n",
    "        if idx in self.poison_indices:\n",
    "            img = self.trigger_func(img)\n",
    "            if self.target_label is not None:\n",
    "                label = self.target_label\n",
    "        return self.transform(img), label\n",
    "\n",
    "CleanPoisonedDTD.__name__ = \"DTD\"\n",
    "\n",
    "template_dataset = CleanPoisonedDTD(\n",
    "    root=dtd_path,\n",
    "    split='train',\n",
    "    download=False,\n",
    "    poison_indices=poison_indices,\n",
    "    trigger_func=add_trigger,\n",
    "    target_label=target_label,\n",
    "    transform=transform_test\n",
    ")\n",
    "template_loader = DataLoader(template_dataset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "# -------------------------\n",
    "# Clean test set loader\n",
    "# -------------------------\n",
    "testset = datasets.DTD(root=dtd_path, split='test', download=False, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "# -------------------------\n",
    "# Backdoor (watermarked) test loader\n",
    "# Create a subset of test samples that are not from the target label,\n",
    "# and apply trigger at evaluation time.\n",
    "# -------------------------\n",
    "non_target_indices = [i for i, (_, l) in enumerate(testset) if l != target_label]\n",
    "backdoor_testset = torch.utils.data.Subset(testset, non_target_indices)\n",
    "\n",
    "def make_backdoor_batch(images):\n",
    "    # Apply the trigger to a batch of tensors by converting to PIL and back\n",
    "    return torch.stack([to_tensor(add_trigger(to_pil_image(img))) for img in images])\n",
    "\n",
    "backdoor_loader = DataLoader(backdoor_testset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Core Model definitions (encoder + projector + AoCL losses)\n",
    "# -------------------------\n",
    "class SupConModel(nn.Module):\n",
    "    def __init__(self, feat_dim=128):\n",
    "        super().__init__()\n",
    "        # Use MobileCLIP's image encoder as feature extractor\n",
    "        self.encoder = model_clip.image_encoder\n",
    "        # A single linear projector maps encoder features to contrastive space (for simplicity, potentially could be improved)\n",
    "        self.projector = nn.Linear(512, feat_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder returns a tensor shaped (batch, feat)\n",
    "        feat = self.encoder(x).squeeze()\n",
    "        proj = F.normalize(self.projector(feat), dim=1)\n",
    "        return proj, feat\n",
    "\n",
    "# -------------------------\n",
    "# Anchor-Only Contrastive Loss (AoCL)\n",
    "# This implementation treats only the paired view as the positive example.\n",
    "# Samples from other classes are treated as negative, while samples from the *same class* are *ignored* (they contribute zero gradient).\n",
    "# Expected inputs:\n",
    "#   features : Tensor with shape [2B, D] (concatenated views: x1_1,...,x1_B,x2_1,...,x2_B)\n",
    "#   labels   : Tensor with shape [B] (original labels for each sample in the batch)\n",
    "# -------------------------\n",
    "class ViewOnlyContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Anchor-only contrastive loss (InfoNCE variant) for Anchor-Only Contrastive Learning (AoCL).\n",
    "\n",
    "    Key behaviors:\n",
    "      - For each anchor view, only its paired view (the other augmentation version of the same sample)\n",
    "        is treated as the positive example.\n",
    "      - Samples from different classes are treated as negatives.\n",
    "      - Other samples from the same class (except the paired view) are explicitly ignored:\n",
    "        they are excluded from both numerator and denominator of the InfoNCE objective\n",
    "        (i.e., they contribute zero gradient).\n",
    "    Expected inputs:\n",
    "      - features: Tensor of shape [2B, D], where the first B rows are view1 and the next B rows are view2.\n",
    "      - labels:   Tensor of shape [B], containing the class label for each sample in the batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        device = features.device\n",
    "        B = labels.shape[0]\n",
    "        N = 2 * B\n",
    "\n",
    "        # Expand labels to match ordering of features: [x1_1,...,x1_B,x2_1,...,x2_B]\n",
    "        labels = labels.repeat(2)\n",
    "\n",
    "        # Compute pairwise similarity matrix scaled by temperature:\n",
    "        # S_ij = (z_i Â· z_j) / tau\n",
    "        sim = torch.div(features @ features.T, self.temperature)\n",
    "\n",
    "        # 1) Exclude self-similarity (diagonal) from softmax by setting it to -inf\n",
    "        mask_self = torch.eye(N, dtype=torch.bool, device=device)\n",
    "        sim.masked_fill_(mask_self, -float('inf'))\n",
    "\n",
    "        # 2) Build positive mask for paired views:\n",
    "        #    for i in [0..B-1], pos_mask[i, i+B] and pos_mask[i+B, i] are True.\n",
    "        idx = torch.arange(B, device=device)\n",
    "        pos_mask = torch.zeros((N, N), dtype=torch.bool, device=device)\n",
    "        pos_mask[idx, idx + B] = True\n",
    "        pos_mask[idx + B, idx] = True\n",
    "\n",
    "        # 3) Construct ignore mask: same-class entries that are neither the anchor nor the paired positive.\n",
    "        #    These entries are excluded (treated as neither positive nor negative).\n",
    "        same_class = labels[:, None] == labels[None, :]\n",
    "        ignore_mask = same_class & (~pos_mask) & (~mask_self)\n",
    "\n",
    "        # 4) Set ignored similarities to -inf so they are omitted from numerator and denominator\n",
    "        sim.masked_fill_(ignore_mask, -float('inf'))\n",
    "\n",
    "        # InfoNCE: numerator is similarity with the single positive; denominator sums over valid entries.\n",
    "        exp_sim = torch.exp(sim)\n",
    "        denom = exp_sim.sum(dim=1)                 # sum over valid (non-ignored, non-self) entries\n",
    "        numer = (exp_sim * pos_mask).sum(dim=1)    # only the paired positive contributes\n",
    "\n",
    "        loss = -torch.log(numer / denom).mean()\n",
    "        return loss\n",
    "\n",
    "# Step 3: Prototype Classifier Construction\n",
    "# -------------------------\n",
    "# Prototype evaluation utilities\n",
    "# Computes prototype centroids from training features and evaluates\n",
    "# classification accuracy and watermark accuracy (VSR) on watermarked examples.\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def prototype_acc_asr(model, trainloader, testloader, backdoor_loader, target_label=0, num_classes=47):\n",
    "    def extract_feats(loader):\n",
    "        feats, labels = [], []\n",
    "        for x, y in loader:\n",
    "            # If dataset returns a pair of views, select the first one for centroid computation\n",
    "            if isinstance(x, (list, tuple)): x = x[0]\n",
    "            x = x.to(device)\n",
    "            _, f = model(x)\n",
    "            feats.append(f.cpu())\n",
    "            labels.append(y)\n",
    "        return torch.cat(feats), torch.cat(labels)\n",
    "\n",
    "    # Extract features for training and test sets\n",
    "    train_feats, train_labels = extract_feats(trainloader)\n",
    "    test_feats, test_labels = extract_feats(testloader)\n",
    "\n",
    "    # For backdoor evaluation, apply trigger at evaluation time and extract features\n",
    "    backdoor_feats = []\n",
    "    for x, _ in backdoor_loader:\n",
    "        x = make_backdoor_batch(x).to(device)\n",
    "        _, f = model(x)\n",
    "        backdoor_feats.append(f.cpu())\n",
    "    backdoor_feats = torch.cat(backdoor_feats)\n",
    "\n",
    "    # Compute class centroids and normalize\n",
    "    centroids = [train_feats[train_labels == c].mean(0) for c in range(num_classes)]\n",
    "    centroids = F.normalize(torch.stack(centroids), dim=1)\n",
    "\n",
    "    def classify(feats):\n",
    "        feats = F.normalize(feats, dim=1)\n",
    "        sim = torch.matmul(feats, centroids.T)\n",
    "        return sim.argmax(dim=1)\n",
    "\n",
    "    acc = (classify(test_feats) == test_labels).float().mean().item()\n",
    "    asr = (classify(backdoor_feats) == target_label).float().mean().item()\n",
    "    return acc, asr\n",
    "\n",
    "# Step 4: Training and Evaluation\n",
    "# -------------------------\n",
    "# Training loop for AoCL\n",
    "# Two-stage schedule:\n",
    "#  1) Warm up projector with the encoder frozen.\n",
    "#  2) Unfreeze encoder and continue training both encoder and projector.\n",
    "# -------------------------\n",
    "model = SupConModel().to(device)\n",
    "criterion = ViewOnlyContrastiveLoss()\n",
    "\n",
    "# Freeze encoder parameters for projector warm-up\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initial evaluation before training\n",
    "acc, asr = prototype_acc_asr(model, template_loader, testloader, backdoor_loader)\n",
    "print(f\"[Initial] ACC: {acc:.4f}, ASR: {asr:.4f}\")\n",
    "\n",
    "optimizer_warm = torch.optim.Adam(model.projector.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "print(\"=> Warm up projector...\")\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for (x1, x2), y in tqdm(trainloader, desc=f\"Epoch {epoch+1:02d}\"):\n",
    "        x = torch.cat([x1, x2], dim=0).to(device)\n",
    "        y = y.to(device)\n",
    "        features, _ = model(x)\n",
    "        loss = criterion(features, y)\n",
    "        optimizer_warm.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_warm.step()\n",
    "        total_loss += loss.item()\n",
    "    acc, asr = prototype_acc_asr(model, template_loader, testloader, backdoor_loader)\n",
    "    print(f\"[Epoch {epoch+1:02d}] Loss: {total_loss/len(trainloader):.4f} | ACC: {acc:.4f} | ASR: {asr:.4f}\")\n",
    "\n",
    "# Unfreeze encoder and fine-tune both encoder and projector with a small LR\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "print(\"=> Unfreezing encoder...\")\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for (x1, x2), y in tqdm(trainloader, desc=f\"Epoch {epoch+1:02d}\"):\n",
    "        x = torch.cat([x1, x2], dim=0).to(device)\n",
    "        y = y.to(device)\n",
    "        features, _ = model(x)\n",
    "        loss = criterion(features, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    acc, asr = prototype_acc_asr(model, template_loader, testloader, backdoor_loader)\n",
    "    print(f\"[Epoch {epoch+1:02d}] Loss: {total_loss/len(trainloader):.4f} | ACC: {acc:.4f} | ASR: {asr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
